{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQZYTjJsrCTJ"
      },
      "source": [
        "  **CS 6375 Assignment-3, Part_2 : K-Means clustering using Jaccard distance**\n",
        "\n",
        "  **Name of students: **\n",
        "  Mounika B(MXB210007)\n",
        "  Saketh Dasavathini(SXD190016)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfVp-Fi1IYlA"
      },
      "source": [
        "#Importing the required libraries\n",
        "\n",
        "import random as rd\n",
        "import re\n",
        "import math\n",
        "import string\n",
        "import urllib.request"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hLpjp4yBLiT"
      },
      "source": [
        "**Data pre-processing of tweets generated related to bbchealth**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f00_2IhNIeE3"
      },
      "source": [
        "def pre_process(url):\n",
        "    # The bbchealth.txt file is uploaded in github at the given location below.\n",
        "    url = 'https://raw.githubusercontent.com/bmounikareddy98/Machine-learning-assignments/main/Assignment3/bbchealth.txt'\n",
        "    file = urllib.request.urlopen(url)\n",
        "    bbc_health_tweets = []\n",
        "    for line in file:\n",
        "       bbc_health_tweets.append(line)\n",
        "    \n",
        "    preprocessed_tweets_list = []\n",
        "\n",
        "    for i in range(len(bbc_health_tweets)):\n",
        "\n",
        "        \n",
        "        bbc_health_tweets[i] = bbc_health_tweets[i].decode('UTF-8') # Converting the byte format of tweet into string, which will automatically removes the \\n character that the end\n",
        "        \n",
        "        # Remove the tweet id and timestamp\n",
        "        bbc_health_tweets[i] = bbc_health_tweets[i][50:]\n",
        "\n",
        "       \n",
        "        bbc_health_tweets[i] = \" \".join(filter(lambda x: x[0] != '@', bbc_health_tweets[i].split()))  # Remove any word that starts with the symbol @\n",
        "\n",
        "        # Remove any URL using the regular expression\n",
        "        bbc_health_tweets[i] = re.sub(r\"http\\S+\", \"\", bbc_health_tweets[i])\n",
        "        bbc_health_tweets[i] = re.sub(r\"www\\S+\", \"\", bbc_health_tweets[i])\n",
        "\n",
        "        # removing the colons from the end of the sentences, after removing url\n",
        "        bbc_health_tweets[i] = bbc_health_tweets[i].strip()\n",
        "        tweet_len = len(bbc_health_tweets[i])\n",
        "        if tweet_len > 0:\n",
        "            if bbc_health_tweets[i][len(bbc_health_tweets[i]) - 1] == ':':\n",
        "                bbc_health_tweets[i] = bbc_health_tweets[i][:len(bbc_health_tweets[i]) - 1]\n",
        "\n",
        "        # Removing any hash-tags symbols\n",
        "        bbc_health_tweets[i] = bbc_health_tweets[i].replace('#', '')\n",
        "\n",
        "        # Convert every word to lowercase\n",
        "        bbc_health_tweets[i] = bbc_health_tweets[i].lower()\n",
        "\n",
        "        # remove punctuations\n",
        "        bbc_health_tweets[i] = bbc_health_tweets[i].translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # trim extra spaces\n",
        "        bbc_health_tweets[i] = \" \".join(bbc_health_tweets[i].split())\n",
        "\n",
        "        # convert each tweet from string type to as list<string> using \" \" as a delimiter\n",
        "        preprocessed_tweets_list.append(bbc_health_tweets[i].split(' '))\n",
        "\n",
        "    file.close()\n",
        "\n",
        "    return preprocessed_tweets_list"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YARBSkHODNMg"
      },
      "source": [
        "**Defining K_means algorithm using Jaccard distance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jods4PdRIjrn"
      },
      "source": [
        "def k_means(bbc_health_tweets, k=4, max_iterations=60):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    \n",
        "    count = 0\n",
        "    map = dict() # creating a dictionary to stor key value pairs\n",
        "    while count < k:\n",
        "        tweet_index = rd.randint(0, len(bbc_health_tweets) - 1) #assigning random tweets as centroids\n",
        "        if tweet_index not in map:\n",
        "            count += 1\n",
        "            map[tweet_index] = True\n",
        "            centroids.append(bbc_health_tweets[tweet_index])\n",
        "\n",
        "    iterations_count = 0\n",
        "    previous_centroids = []\n",
        "\n",
        "    # running the iterations until they are not converged or till the max iterations count in not reached\n",
        "    while (is_k_means_converged(previous_centroids, centroids)) == False and (iterations_count < max_iterations):\n",
        "\n",
        "        print(\"running iteration \" + str(iterations_count))\n",
        "\n",
        "        # assignment, assign tweets to the closest centroids\n",
        "        clusters = assign_the_cluster(bbc_health_tweets, centroids)\n",
        "\n",
        "        # to check if k-means converges, keep track of previous_centroids\n",
        "        previous_centroids = centroids\n",
        "\n",
        "        # update, update centroid based on clusters formed\n",
        "        centroids = updation_of_centroids(clusters)\n",
        "        iterations_count = iterations_count + 1\n",
        "\n",
        "    if (iterations_count == max_iterations):\n",
        "        print(\"K Means is not converged as maximum iterations count is reached\")\n",
        "    else:\n",
        "        print(\"K Means is converged\")\n",
        "\n",
        "    SSE = compute_SSE(clusters)\n",
        "\n",
        "    return clusters, SSE"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyPYiRbsfj6H"
      },
      "source": [
        "**Checking for the convergence of k-means**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egM6mK0GInKF"
      },
      "source": [
        "def is_k_means_converged(previous_centroids, new_centroids):\n",
        "\n",
        "    # If lengths are not equal, we will return false\n",
        "    if len(previous_centroids) != len(new_centroids):\n",
        "        return False\n",
        "\n",
        "    #Going over through each entry of clusters and checking if they are same\n",
        "    for c in range(len(new_centroids)):\n",
        "        if \" \".join(new_centroids[c]) != \" \".join(previous_centroids[c]):\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqcy1gvdgyrA"
      },
      "source": [
        "**Assigning tweets to clusters according to nearest centroid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW5fWcFEIr67"
      },
      "source": [
        "def assign_the_cluster(bbc_health_tweets, centroids):\n",
        "\n",
        "    clusters = dict()\n",
        "\n",
        "    # for every health tweet, calculating closest centroid and assigning the tweet to that cluster\n",
        "    for i in range(len(bbc_health_tweets)):\n",
        "        min_distance = math.inf\n",
        "        cluster_index = -1;\n",
        "        for c in range(len(centroids)):\n",
        "            dis = getJaccardDistance(centroids[c], bbc_health_tweets[i])\n",
        "            # look for a closest centroid for a tweet\n",
        "\n",
        "            if centroids[c] == bbc_health_tweets[i]:\n",
        "                cluster_index = c\n",
        "                min_distance = 0\n",
        "                break\n",
        "\n",
        "            if dis < min_distance:\n",
        "                cluster_index = c\n",
        "                min_distance = dis\n",
        "\n",
        "        # If we dont have anything in common, we will randomly assign the tweet to cluster\n",
        "        if min_distance == 1:\n",
        "            cluster_index = rd.randint(0, len(centroids) - 1)\n",
        "\n",
        "        # assigning the closest centroid to a tweet\n",
        "        clusters.setdefault(cluster_index, []).append([bbc_health_tweets[i]])\n",
        "       \n",
        "        last_tweet_index = len(clusters.setdefault(cluster_index, [])) - 1\n",
        "        clusters.setdefault(cluster_index, [])[last_tweet_index].append(min_distance)\n",
        "\n",
        "    return clusters"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CsJKIwMIvyp"
      },
      "source": [
        "def updation_of_centroids(clusters):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    # iterate through each cluster and check for a health tweet with closest distance and with all other tweets in the same cluster\n",
        "    # select that tweet as the centroid for the cluster\n",
        "    for i in range(len(clusters)):\n",
        "        min_distance_sum = math.inf\n",
        "        centroid_index = -1\n",
        "\n",
        "        # to avoid redundant calculations\n",
        "        min_distance_dp = []\n",
        "\n",
        "        for t1 in range(len(clusters[i])):\n",
        "            min_distance_dp.append([])\n",
        "            distance_sum = 0\n",
        "            # get distances sum for every of tweet t1 with every tweet t2 in a same cluster\n",
        "            for t2 in range(len(clusters[i])):\n",
        "                if t1 != t2:\n",
        "                    if t2 < t1:\n",
        "                        distance = min_distance_dp[t2][t1]\n",
        "                    else:\n",
        "                        distance = getJaccardDistance(clusters[i][t1][0], clusters[i][t2][0])\n",
        "\n",
        "                    min_distance_dp[t1].append(distance)\n",
        "                    distance_sum += distance\n",
        "                else:\n",
        "                    min_distance_dp[t1].append(0)\n",
        "\n",
        "            # The tweet with the minimum distances sum as the centroid for the cluster is selected\n",
        "            if distance_sum < min_distance_sum:\n",
        "                min_distance_sum = distance_sum\n",
        "                centroid_index = t1\n",
        "\n",
        "        # appending the selected tweet to the centroid list\n",
        "        centroids.append(clusters[i][centroid_index][0])\n",
        "\n",
        "    return centroids"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhnT1CWUi7XL"
      },
      "source": [
        "**Calculating the Jaccard distance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1KwRq8YI1P4"
      },
      "source": [
        "def getJaccardDistance(tweet1, tweet2):\n",
        "\n",
        "    # get the intersection\n",
        "    intersection = set(tweet1).intersection(tweet2)\n",
        "\n",
        "    # get the union\n",
        "    union = set().union(tweet1, tweet2)\n",
        "\n",
        "    # return the jaccard distance\n",
        "    return 1 - (len(intersection) / len(union))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lOz1njxmVRs"
      },
      "source": [
        "**Compute the SSE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4_IhDxgl7Qt"
      },
      "source": [
        "def compute_SSE(clusters):\n",
        "\n",
        "    sse = 0\n",
        "    # iterate through each cluster 'c' and compute the sum of square of distances of the tweet from it's centroid\n",
        "    for c in range(len(clusters)):\n",
        "        for t in range(len(clusters[c])):\n",
        "            sse = sse + (clusters[c][t][1] * clusters[c][t][1])\n",
        "\n",
        "    return sse"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUn4tDLGoBbV"
      },
      "source": [
        "**Main function consisting of all the function calls above **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAmC6RrmmC_-",
        "outputId": "a21d1877-bd7c-42e9-f177-a2ddaaa186d9"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    data_url = 'https://raw.githubusercontent.com/bmounikareddy98/Machine-learning-assignments/main/Assignment3/bbchealth.txt'\n",
        "\n",
        "    bbc_health_tweets = pre_process(data_url)\n",
        "\n",
        "    # default number of experiments to be performed using different values of k\n",
        "    experiments = 5\n",
        "\n",
        "    # initial value of k for K-means\n",
        "    k = 3\n",
        "\n",
        "    # for every experiment 'e', run K-means\n",
        "    for e in range(experiments):\n",
        "\n",
        "        print(\"Running K means for experiment no. \" + str((e + 1)) + \" for k = \" + str(k))\n",
        "\n",
        "        clusters, sse = k_means(bbc_health_tweets, k)\n",
        "\n",
        "        # for every cluster 'c', print size of each cluster\n",
        "        for c in range(len(clusters)):\n",
        "            print(str(c+1) + \": \", str(len(clusters[c])) + \" tweets\")\n",
        "           \n",
        "        print(\"--> SSE : \" + str(sse))\n",
        "        print('\\n')\n",
        "\n",
        "        # incrementing k after every experiment\n",
        "        k = k + 1"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K means for experiment no. 1 for k = 3\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "running iteration 2\n",
            "K Means is converged\n",
            "1:  907 tweets\n",
            "2:  1569 tweets\n",
            "3:  1453 tweets\n",
            "--> SSE : 3405.1838451701815\n",
            "\n",
            "\n",
            "Running K means for experiment no. 2 for k = 4\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "K Means is converged\n",
            "1:  1197 tweets\n",
            "2:  714 tweets\n",
            "3:  1248 tweets\n",
            "4:  770 tweets\n",
            "--> SSE : 3410.1369310634163\n",
            "\n",
            "\n",
            "Running K means for experiment no. 3 for k = 5\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "running iteration 2\n",
            "K Means is converged\n",
            "1:  477 tweets\n",
            "2:  1531 tweets\n",
            "3:  624 tweets\n",
            "4:  682 tweets\n",
            "5:  615 tweets\n",
            "--> SSE : 3327.0019087411424\n",
            "\n",
            "\n",
            "Running K means for experiment no. 4 for k = 6\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "running iteration 2\n",
            "K Means is converged\n",
            "1:  828 tweets\n",
            "2:  280 tweets\n",
            "3:  644 tweets\n",
            "4:  1115 tweets\n",
            "5:  472 tweets\n",
            "6:  590 tweets\n",
            "--> SSE : 3327.1853256754716\n",
            "\n",
            "\n",
            "Running K means for experiment no. 5 for k = 7\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "K Means is converged\n",
            "1:  433 tweets\n",
            "2:  890 tweets\n",
            "3:  358 tweets\n",
            "4:  711 tweets\n",
            "5:  453 tweets\n",
            "6:  680 tweets\n",
            "7:  404 tweets\n",
            "--> SSE : 3280.3730407042485\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4cV_eCZMVgH",
        "outputId": "85a6e0ad-a745-4e31-b3fd-0a113ca9fc51"
      },
      "source": [
        "import urllib.request\n",
        "data_url = 'https://raw.githubusercontent.com/bmounikareddy98/Machine-learning-assignments/main/Assignment3/bbchealth.txt'\n",
        " #pre_process_tweets(data_url)\n",
        "#f = open(data_url, \"r\", encoding=\"utf8\")\n",
        "file = urllib.request.urlopen(data_url)\n",
        "\n",
        "tweets=[]\n",
        "\"\"\"for line in file:\n",
        "  print(line)\n",
        "  tweets.append(line)\"\"\"\n",
        "for line in file:\n",
        "  tweets.append(line)\n",
        "\"\"\"for line in file:\n",
        "\tdecoded_line = line.decode(\"utf-8\")\n",
        "  tweets.append(line)\n",
        "\t#print(decoded_line)\n",
        "  \n",
        "file.read(\"r\")\"\"\"\n",
        "\n",
        "#tweets=list(file)\n",
        "#tweets[0]= tweets[0].replace('\\n',\"\")\n",
        "tweets[0] = tweets[0].decode('UTF-8')\n",
        "print(type(tweets[0]))\n",
        "print(tweets[0])\n",
        "print(type(tweets[0]))\n",
        "#str(b, encoding)\n",
        "\n",
        "#print(a)\n",
        "#for line in file:\n",
        "  #print(line)\n",
        "\n",
        "    \n",
        "#print(file.read)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "585978391360221184|Thu Apr 09 01:31:50 +0000 2015|Breast cancer risk test devised http://bbc.in/1CimpJF\r\n",
            "\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSNAPZF_sBzd",
        "outputId": "122faf2f-653b-4ca5-c72d-6fef1a6a50e1"
      },
      "source": [
        "#tweets[0] = tweets[0].strip('\\n')\n",
        "print(tweets[0])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'585978391360221184|Thu Apr 09 01:31:50 +0000 2015|Breast cancer risk test devised http://bbc.in/1CimpJF\\r\\n'\n"
          ]
        }
      ]
    }
  ]
}